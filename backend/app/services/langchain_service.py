from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables.history import RunnableWithMessageHistory
from .memory_service import FirestoreChatMessageHistory
from ..config.settings import settings

# 1. Initialize the LLM
llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=settings.GOOGLE_API_KEY, temperature=0.7)

# 2. Create the Prompt Template
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful and friendly assistant. Answer the user's questions clearly and concisely."),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)

# 3. Create the primary Conversation Chain
conversation_chain = prompt | llm

# 4. Define the history factory function. This remains simple.
def get_session_history(session_id: str, user_id: str) -> FirestoreChatMessageHistory:
    return FirestoreChatMessageHistory(conversation_id=session_id, user_id=user_id)

# 5. THIS IS THE FINAL FIX: Create a chain that correctly handles user_id
# This chain does two things:
# - It takes the original input (`question` and `user_id`).
# - It assigns a `history` key to the dictionary. The value is generated by
#   calling our `get_session_history` function with the correct `user_id`
#   and the `session_id` from the runtime config.
contextualize_chain = RunnablePassthrough.assign(
    history=lambda x: get_session_history(
        session_id=x["configurable"]["session_id"],
        user_id=x["input"]["user_id"]
    ).messages
)

# 6. Combine the chains
# The flow is now:
# 1. The input (question, user_id) goes into contextualize_chain.
# 2. contextualize_chain fetches the history and adds it to the dictionary.
# 3. The full dictionary (question, user_id, history) is passed to the main conversation_chain.
chain_with_history_and_user = contextualize_chain | conversation_chain

# 7. Wrap the final chain with history management
# This wrapper's job is now ONLY to save the new messages to the history
# that was already fetched and used by our `contextualize_chain`.
chain_with_history = RunnableWithMessageHistory(
    chain_with_history_and_user,
    lambda session_id, **kwargs: get_session_history(session_id, user_id=kwargs["user_id"]),
    input_messages_key="question",
    history_messages_key="history",
)